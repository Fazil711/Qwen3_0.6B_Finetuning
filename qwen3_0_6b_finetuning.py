# -*- coding: utf-8 -*-
"""Qwen3_0.6B_Finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lk6cQVzMEZpLVPmByPZWeXgJY1Y7K7fS
"""

!pip install --no-deps "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps "xformers<0.0.29" "trl<0.9.0" peft accelerate bitsandbytes

!pip install unsloth_zoo

from unsloth import FastLanguageModel
import torch

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen3-0.6B-bnb-4bit",
    max_seq_length = 2048,
    load_in_4bit = True,
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 32,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 32,
    lora_dropout = 0.05,
    bias = "none",
)

from datasets import load_dataset

dataset = load_dataset("gsm8k", "main", split = "train")

math_prompt = """<think>
{}
</think>
{}"""

def formatting_prompts_func(examples):
    instructions = examples["question"]
    outputs      = examples["answer"]
    texts = []
    for instruction, output in zip(instructions, outputs):
        text = math_prompt.format(instruction, output)
        texts.append(text)
    return { "text" : texts, }

dataset = dataset.map(formatting_prompts_func, batched = True)

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = 2048,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        max_steps = 300,
        learning_rate = 1e-4,
        lr_scheduler_type = "cosine",
        weight_decay = 0.01,
        optim = "adamw_8bit",
        output_dir = "outputs_high_perf",
    ),
)

trainer.train()

model.save_pretrained_gguf(
    "/content/drive/MyDrive/qwen3_math_gguf",
    tokenizer,
    quantization_method = "q4_k_m"
)

!pip install llama-cpp-python \
  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121

from llama_cpp import Llama
import os

gguf_files = [f for f in os.listdir('.') if f.endswith('.gguf')]
model_path = gguf_files[0] if gguf_files else "/content/drive/MyDrive/Qwen3-0.6B.Q4_K_M.gguf"

print(f"Loading model: {model_path}")

llm = Llama(
    model_path=model_path,
    n_gpu_layers=-1,
    n_ctx=2048,
)

prompt = "Question: If a train travels at 60 mph for 2.5 hours, how far does it go? Please reason step by step, and put your final answer within \\boxed{}."

output = llm(
    f"user\n{prompt}\nassistant\n<think>\n",
    max_tokens=512,
    stop=["</think>", "\nuser", "\nassistant"],
    temperature=0.6,
    echo=False
)

reasoning = output["choices"][0]["text"]
print(f"--- REASONING ---\n<think>\n{reasoning}\n</think>")

answer_output = llm(
    f"user\n{prompt}\nassistant\n<think>\n{reasoning}\n</think>\n",
    max_tokens=128,
    temperature=0.6,
)
print(f"\n--- FINAL ANSWER ---\n{answer_output['choices'][0]['text']}")

!pip install lm-eval[hf,anthropic]

from huggingface_hub import hf_hub_download
import shutil
import os

drive_path = "/content/drive/MyDrive/"

files_to_download = ["config.json", "tokenizer.json", "tokenizer_config.json"]

for file in files_to_download:
    local_file = hf_hub_download(repo_id="Qwen/Qwen3-0.6B", filename=file)
    shutil.copy(local_file, os.path.join(drive_path, file))

print("Metadata files copied to Drive. Now run the benchmark again pointing to the folder:")

!pip install gguf>=0.10.0

!lm_eval --model hf \
    --model_args "pretrained=/content/drive/MyDrive/qwen3_math_gguf,gguf_file=/content/drive/MyDrive/Qwen3-0.6B.Q4_K_M_2.gguf" \
    --tasks gsm8k \
    --device cuda:0 \
    --batch_size 8 \
    --limit 50 \
    --gen_kwargs "do_sample=True,temperature=0.6,max_gen_toks=512,top_p=0.95,repetition_penalty=1.1"